{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from typing import Callable, Sequence\n",
    "from jax import random, numpy as jnp\n",
    "import flax\n",
    "from flax import linen as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Dense(features=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dense(\n",
       "    # attributes\n",
       "    features = 5\n",
       "    use_bias = True\n",
       "    dtype = None\n",
       "    param_dtype = float32\n",
       "    precision = None\n",
       "    kernel_init = init\n",
       "    bias_init = zeros\n",
       "    dot_general = None\n",
       "    dot_general_cls = None\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'bias': (5,), 'kernel': (10, 5)}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key1, key2 = random.split(random.key(0))\n",
    "x = random.normal(key1, (10,)) # Dummy input data\n",
    "params = model.init(key2, x) # Initialization call\n",
    "jax.tree_util.tree_map(lambda x: x.shape, params) # Checking output shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-1.3721197 ,  0.61131513,  0.6442838 ,  2.2192965 , -1.1271117 ],      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(params, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (20, 10) ; y shape: (20, 5)\n"
     ]
    }
   ],
   "source": [
    "# Set problem dimensions.\n",
    "n_samples = 20\n",
    "x_dim = 10\n",
    "y_dim = 5\n",
    "\n",
    "# Generate random ground truth W and b.\n",
    "key = random.key(0)\n",
    "k1, k2 = random.split(key)\n",
    "W = random.normal(k1, (x_dim, y_dim))\n",
    "b = random.normal(k2, (y_dim,))\n",
    "# Store the parameters in a FrozenDict pytree.\n",
    "true_params = flax.core.freeze({'params': {'bias': b, 'kernel': W}})\n",
    "\n",
    "# Generate samples with additional noise.\n",
    "key_sample, key_noise = random.split(k1)\n",
    "x_samples = random.normal(key_sample, (n_samples, x_dim))\n",
    "y_samples = jnp.dot(x_samples, W) + b + 0.1 * random.normal(key_noise,(n_samples, y_dim))\n",
    "print('x shape:', x_samples.shape, '; y shape:', y_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as JAX version but using model.apply().\n",
    "@jax.jit\n",
    "def mse(params, x_batched, y_batched):\n",
    "    # Define the squared loss for a single pair (x,y)\n",
    "    def squared_error(x, y):\n",
    "        pred = model.apply(params, x)\n",
    "        return jnp.inner(y-pred, y-pred) / 2.0\n",
    "    # Vectorize the previous to compute the average of the loss on all samples.\n",
    "    return jnp.mean(jax.vmap(squared_error)(x_batched,y_batched), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for \"true\" W,b:  0.023639798\n",
      "Loss step 0:  35.343876\n",
      "Loss step 10:  0.51505065\n",
      "Loss step 20:  0.114045195\n",
      "Loss step 30:  0.039395172\n",
      "Loss step 40:  0.01994014\n",
      "Loss step 50:  0.01421761\n",
      "Loss step 60:  0.012428714\n",
      "Loss step 70:  0.011851465\n",
      "Loss step 80:  0.011662135\n",
      "Loss step 90:  0.011599515\n",
      "Loss step 100:  0.011578727\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.3  # Gradient step size.\n",
    "print('Loss for \"true\" W,b: ', mse(true_params, x_samples, y_samples))\n",
    "loss_grad_fn = jax.value_and_grad(mse)\n",
    "\n",
    "@jax.jit\n",
    "def update_params(params, learning_rate, grads):\n",
    "    params = jax.tree_util.tree_map(\n",
    "        lambda p, g: p - learning_rate * g, params, grads)\n",
    "    return params\n",
    "\n",
    "for i in range(101):\n",
    "    # Perform one gradient update.\n",
    "    loss_val, grads = loss_grad_fn(params, x_samples, y_samples)\n",
    "    params = update_params(params, learning_rate, grads)\n",
    "    if i % 10 == 0:\n",
    "        print(f'Loss step {i}: ', loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "tx = optax.adam(learning_rate=learning_rate)\n",
    "opt_state = tx.init(params)\n",
    "loss_grad_fn = jax.value_and_grad(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss step 0:  0.01157765\n",
      "Loss step 10:  0.2614\n",
      "Loss step 20:  0.07683634\n",
      "Loss step 30:  0.036484905\n",
      "Loss step 40:  0.022029908\n",
      "Loss step 50:  0.016185088\n",
      "Loss step 60:  0.01299824\n",
      "Loss step 70:  0.012026562\n",
      "Loss step 80:  0.011765008\n",
      "Loss step 90:  0.0116460025\n",
      "Loss step 100:  0.0115856035\n"
     ]
    }
   ],
   "source": [
    "for i in range(101):\n",
    "    loss_val, grads = loss_grad_fn(params, x_samples, y_samples)\n",
    "    updates, opt_state = tx.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    if i % 10 == 0:\n",
    "        print(f'Loss step {i}: ', loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict output\n",
      "{'params': {'bias': Array([-1.4555818 , -2.0278208 ,  2.0790894 ,  1.2186159 , -0.99807364],      dtype=float32), 'kernel': Array([[ 1.0098742 ,  0.18935174,  0.04456387, -0.9280217 ,  0.3478275 ],\n",
      "       [ 1.7298428 ,  0.9879267 ,  1.1640524 ,  1.1006035 , -0.10651099],\n",
      "       [-1.2029499 ,  0.28633362,  1.4155992 ,  0.11869171, -1.3141348 ],\n",
      "       [-1.1941439 , -0.18958223,  0.03415466,  1.3169502 ,  0.08061308],\n",
      "       [ 0.13851956,  1.3712972 , -1.3187258 ,  0.53151757, -2.2405186 ],\n",
      "       [ 0.56294185,  0.8122366 ,  0.317533  ,  0.5345454 ,  0.9049949 ],\n",
      "       [-0.37925696,  1.7410626 ,  1.0790431 , -0.5039805 ,  0.92827296],\n",
      "       [ 0.97064775, -1.3153123 ,  0.33682504,  0.8099361 , -1.2018671 ],\n",
      "       [ 1.0194423 , -0.62024164,  1.0818756 , -1.8389823 , -0.45808458],\n",
      "       [-0.6436587 ,  0.45668206, -1.1329095 , -0.6853872 ,  0.1683104 ]],      dtype=float32)}}\n",
      "Bytes output\n",
      "b'\\x81\\xa6params\\x82\\xa4bias\\xc7!\\x01\\x93\\x91\\x05\\xa7float32\\xc4\\x14\\x81P\\xba\\xbf\\xd1\\xc7\\x01\\xc0\\xcd\\x0f\\x05@\\x9b\\xfb\\x9b?\\xc1\\x81\\x7f\\xbf\\xa6kernel\\xc7\\xd6\\x01\\x93\\x92\\n\\x05\\xa7float32\\xc4\\xc8\\x8fC\\x81?l\\xe5A>\\x9a\\x886=\\xd5\\x92m\\xbfr\\x16\\xb2>}k\\xdd?\\xc4\\xe8|?\\xab\\xff\\x94?\\x93\\xe0\\x8c?o\"\\xda\\xbdC\\xfa\\x99\\xbfR\\x9a\\x92>[2\\xb5?\\xa3\\x14\\xf3=\\x925\\xa8\\xbf\\xb5\\xd9\\x98\\xbf\\xd8!B\\xbe\\xc2\\xe5\\x0b=\\xd3\\x91\\xa8?x\\x18\\xa5=\\x12\\xd8\\r>\\xab\\x86\\xaf?\\x02\\xcc\\xa8\\xbf\\x89\\x11\\x08?\\xa8d\\x0f\\xc0\\xf5\\x1c\\x10?\\xbd\\xeeO?\\xaf\\x93\\xa2>\\xf8\\xd7\\x08?\\xbf\\xadg?\\xf8-\\xc2\\xbe$\\xdb\\xde?\\x16\\x1e\\x8a?\\xde\\x04\\x01\\xbfL\\xa3m?_|x?\\'\\\\\\xa8\\xbfUt\\xac>\\xf9WO?\\xc8\\xd6\\x99\\xbf\\x16}\\x82?(\\xc8\\x1e\\xbf\\xe6z\\x8a?\\xc6c\\xeb\\xbf\\x10\\x8a\\xea\\xbe\\xd1\\xc6$\\xbf;\\xd2\\xe9>.\\x03\\x91\\xbf\\x89u/\\xbf\\x90Y,>'\n"
     ]
    }
   ],
   "source": [
    "from flax import serialization\n",
    "bytes_output = serialization.to_bytes(params)\n",
    "dict_output = serialization.to_state_dict(params)\n",
    "print('Dict output')\n",
    "print(dict_output)\n",
    "print('Bytes output')\n",
    "print(bytes_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'bias': array([-1.4555818 , -2.0278208 ,  2.0790894 ,  1.2186159 , -0.99807364],\n",
       "        dtype=float32),\n",
       "  'kernel': array([[ 1.0098742 ,  0.18935174,  0.04456387, -0.9280217 ,  0.3478275 ],\n",
       "         [ 1.7298428 ,  0.9879267 ,  1.1640524 ,  1.1006035 , -0.10651099],\n",
       "         [-1.2029499 ,  0.28633362,  1.4155992 ,  0.11869171, -1.3141348 ],\n",
       "         [-1.1941439 , -0.18958223,  0.03415466,  1.3169502 ,  0.08061308],\n",
       "         [ 0.13851956,  1.3712972 , -1.3187258 ,  0.53151757, -2.2405186 ],\n",
       "         [ 0.56294185,  0.8122366 ,  0.317533  ,  0.5345454 ,  0.9049949 ],\n",
       "         [-0.37925696,  1.7410626 ,  1.0790431 , -0.5039805 ,  0.92827296],\n",
       "         [ 0.97064775, -1.3153123 ,  0.33682504,  0.8099361 , -1.2018671 ],\n",
       "         [ 1.0194423 , -0.62024164,  1.0818756 , -1.8389823 , -0.45808458],\n",
       "         [-0.6436587 ,  0.45668206, -1.1329095 , -0.6853872 ,  0.1683104 ]],\n",
       "        dtype=float32)}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serialization.from_bytes(params, bytes_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized parameter shapes:\n",
      " {'params': {'layers_0': {'bias': (3,), 'kernel': (4, 3)}, 'layers_1': {'bias': (4,), 'kernel': (3, 4)}, 'layers_2': {'bias': (5,), 'kernel': (4, 5)}}}\n",
      "output:\n",
      " [[ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.00723787 -0.00810345 -0.0255093   0.02151708 -0.01261237]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "class ExplicitMLP(nn.Module):\n",
    "    features: Sequence[int]\n",
    "\n",
    "    def setup(self):\n",
    "        # we automatically know what to do with lists, dicts of submodules\n",
    "        self.layers = [nn.Dense(feat) for feat in self.features]\n",
    "        # for single submodules, we would just write:\n",
    "        # self.layer1 = nn.Dense(feat1)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        for i, lyr in enumerate(self.layers):\n",
    "            x = lyr(x)\n",
    "            if i != len(self.layers) - 1:\n",
    "                x = nn.relu(x)\n",
    "        return x\n",
    "\n",
    "key1, key2 = random.split(random.key(0), 2)\n",
    "x = random.uniform(key1, (4,4))\n",
    "\n",
    "model = ExplicitMLP(features=[3,4,5])\n",
    "params = model.init(key2, x)\n",
    "y = model.apply(params, x)\n",
    "\n",
    "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(params)))\n",
    "print('output:\\n', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"ExplicitMLP\" object has no attribute \"layers\". If \"layers\" is defined in '.setup()', remember these fields are only accessible from inside 'init' or 'apply'.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    y = model(x) # Returns an error\n",
    "except AttributeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized parameter shapes:\n",
      " {'params': {'layers_0': {'bias': (3,), 'kernel': (4, 3)}, 'layers_1': {'bias': (4,), 'kernel': (3, 4)}, 'layers_2': {'bias': (5,), 'kernel': (4, 5)}}}\n",
      "output:\n",
      " [[ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.00723787 -0.00810345 -0.0255093   0.02151708 -0.01261237]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    features: Sequence[int]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        for i, feat in enumerate(self.features):\n",
    "            x = nn.Dense(feat, name=f'layers_{i}')(x)\n",
    "            if i != len(self.features) - 1:\n",
    "                x = nn.relu(x)\n",
    "        # providing a name is optional though!\n",
    "        # the default autonames would be \"Dense_0\", \"Dense_1\", ...\n",
    "        return x\n",
    "\n",
    "key1, key2 = random.split(random.key(0), 2)\n",
    "x = random.uniform(key1, (4,4))\n",
    "\n",
    "model = SimpleMLP(features=[3,4,5])\n",
    "params = model.init(key2, x)\n",
    "y = model.apply(params, x)\n",
    "\n",
    "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(params)))\n",
    "print('output:\\n', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized parameters:\n",
      " {'params': {'kernel': Array([[ 0.61506   , -0.22728713,  0.60547006],\n",
      "       [-0.2961799 ,  1.1232013 , -0.879759  ],\n",
      "       [-0.35162622,  0.38064912,  0.68932474],\n",
      "       [-0.1151355 ,  0.04567899, -1.091212  ]], dtype=float32), 'bias': Array([0., 0., 0.], dtype=float32)}}\n",
      "output:\n",
      " [[-0.029962    1.102088   -0.6660265 ]\n",
      " [-0.31092793  0.6323942  -0.5367881 ]\n",
      " [ 0.0142401   0.9424717  -0.6356147 ]\n",
      " [ 0.36818963  0.35865188 -0.00459227]]\n"
     ]
    }
   ],
   "source": [
    "class SimpleDense(nn.Module):\n",
    "    features: int\n",
    "    kernel_init: Callable = nn.initializers.lecun_normal()\n",
    "    bias_init: Callable = nn.initializers.zeros_init()\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs):\n",
    "        kernel = self.param('kernel',\n",
    "                            self.kernel_init, # Initialization function\n",
    "                            (inputs.shape[-1], self.features))  # shape info.\n",
    "        y = jnp.dot(inputs, kernel)\n",
    "        bias = self.param('bias', self.bias_init, (self.features,))\n",
    "        y = y + bias\n",
    "        return y\n",
    "\n",
    "key1, key2 = random.split(random.key(0), 2)\n",
    "x = random.uniform(key1, (4,4))\n",
    "\n",
    "model = SimpleDense(features=3)\n",
    "params = model.init(key2, x)\n",
    "y = model.apply(params, x)\n",
    "\n",
    "print('initialized parameters:\\n', params)\n",
    "print('output:\\n', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized variables:\n",
      " {'batch_stats': {'mean': Array([0., 0., 0., 0., 0.], dtype=float32)}, 'params': {'bias': Array([0., 0., 0., 0., 0.], dtype=float32)}}\n",
      "updated state:\n",
      " {'batch_stats': {'mean': Array([[0.01, 0.01, 0.01, 0.01, 0.01]], dtype=float32)}}\n"
     ]
    }
   ],
   "source": [
    "class BiasAdderWithRunningMean(nn.Module):\n",
    "    decay: float = 0.99\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # easy pattern to detect if we're initializing via empty variable tree\n",
    "        is_initialized = self.has_variable('batch_stats', 'mean')\n",
    "        ra_mean = self.variable('batch_stats', 'mean',\n",
    "                                lambda s: jnp.zeros(s),\n",
    "                                x.shape[1:])\n",
    "        bias = self.param('bias', lambda rng, shape: jnp.zeros(shape), x.shape[1:])\n",
    "        if is_initialized:\n",
    "            ra_mean.value = self.decay * ra_mean.value + (1.0 - self.decay) * jnp.mean(x, axis=0, keepdims=True)\n",
    "\n",
    "        return x - ra_mean.value + bias\n",
    "\n",
    "\n",
    "key1, key2 = random.split(random.key(0), 2)\n",
    "x = jnp.ones((10,5))\n",
    "model = BiasAdderWithRunningMean()\n",
    "variables = model.init(key1, x)\n",
    "print('initialized variables:\\n', variables)\n",
    "y, updated_state = model.apply(variables, x, mutable=['batch_stats'])\n",
    "print('updated state:\\n', updated_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated state:\n",
      " {'batch_stats': {'mean': Array([[0.01, 0.01, 0.01, 0.01, 0.01]], dtype=float32)}}\n",
      "updated state:\n",
      " {'batch_stats': {'mean': Array([[0.0299, 0.0299, 0.0299, 0.0299, 0.0299]], dtype=float32)}}\n",
      "updated state:\n",
      " {'batch_stats': {'mean': Array([[0.059601, 0.059601, 0.059601, 0.059601, 0.059601]], dtype=float32)}}\n"
     ]
    }
   ],
   "source": [
    "for val in [1.0, 2.0, 3.0]:\n",
    "    x = val * jnp.ones((10,5))\n",
    "    y, updated_state = model.apply(variables, x, mutable=['batch_stats'])\n",
    "    old_state, params = flax.core.pop(variables, 'params')\n",
    "    variables = flax.core.freeze({'params': params, **updated_state})\n",
    "    print('updated state:\\n', updated_state) # Shows only the mutable part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated state:  {'batch_stats': {'mean': Array([[0.01, 0.01, 0.01, 0.01, 0.01]], dtype=float32)}}\n",
      "Updated state:  {'batch_stats': {'mean': Array([[0.0199, 0.0199, 0.0199, 0.0199, 0.0199]], dtype=float32)}}\n",
      "Updated state:  {'batch_stats': {'mean': Array([[0.029701, 0.029701, 0.029701, 0.029701, 0.029701]], dtype=float32)}}\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "@partial(jax.jit, static_argnums=(0, 1))\n",
    "def update_step(tx, apply_fn, x, opt_state, params, state):\n",
    "\n",
    "    def loss(params):\n",
    "        y, updated_state = apply_fn({'params': params, **state},\n",
    "                                    x, mutable=list(state.keys()))\n",
    "        _l = ((x - y) ** 2).sum()\n",
    "        return _l, updated_state\n",
    "\n",
    "    (_l, state), grads = jax.value_and_grad(loss, has_aux=True)(params)\n",
    "    updates, opt_state = tx.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return opt_state, params, state\n",
    "\n",
    "x = jnp.ones((10,5))\n",
    "variables = model.init(random.key(0), x)\n",
    "state, params = flax.core.pop(variables, 'params')\n",
    "del variables\n",
    "tx = optax.sgd(learning_rate=0.02)\n",
    "opt_state = tx.init(params)\n",
    "\n",
    "for _ in range(3):\n",
    "    opt_state, params, state = update_step(tx, model.apply, x, opt_state, params, state)\n",
    "    print('Updated state: ', state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
